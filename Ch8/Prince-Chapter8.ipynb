{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping criteria met after 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import utilities as utils\n",
    "reload(utils)\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common import multivariate_student\n",
    "reload(multivariate_student)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "The conditional density for the discriminative model is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "Pr(w \\ | \\ \\mathbf{X} ) &=& \\text{Beta}_{w} [\\alpha, \\alpha] \\\\\n",
    "&=& \\frac{\\Gamma[2 \\alpha]}{\\Gamma[\\alpha]^2} w^{\\alpha-1}(1-w)^{\\alpha-1}, \n",
    "\\end{eqnarray}\n",
    "\n",
    "where the given constraint $\\alpha=\\beta$ has been used.\n",
    "\n",
    "The parameter $\\alpha \\in \\mathbb{R}^+$ is said to depend on the data, so one approach would be to setup a linear mapping from $\\mathbb{R}^D$ to $\\mathbb{R}^+$ as\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha = \\frac{1}{I} \\sum_{i=1}^I f(\\mathbf{\\phi}^T \\mathbf{x'}_i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\mathbf{x'}$ includes the constant 1 in the first position to allow training the bias term $\\phi_0$.  The continuously differentiable function $f$ is chosen so that $\\alpha$ can be enforced to be positive.  One option for $f$ would be a superposition of radial basis functions.\n",
    "\n",
    "Next, find the log-likelihood; this will provide the objective function to be maximized.  Assuming the datapoints are independent, it follows that:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log L &=& \\sum_{i=1}^I I \\log \\Gamma [2 \\alpha ] - 2 I \\log \\Gamma [ \\alpha ] + (\\alpha-1) \\sum_{i=1}^I \\bigl ( \\log w_i + \\log [1 - w_i ] \\bigr ) \n",
    "\\end{eqnarray}\n",
    "\n",
    "Now, the model for $\\alpha$ can be plugged in to the log-likelihood and the resulting equation can be differentiated with respect to the $\\phi_k$'s, with $k \\in {0, \\dots, D}$.  The maximum likelihood will occur where all derivatives vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Start with the conditional probability density:\n",
    "\\begin{eqnarray}\n",
    "Pr(\\mathbf{w} \\ | \\ \\mathbf{X}) &=& \\text{Stud}_\\mathbf{w} \\bigl [ \\mathbf{X}^T \\mathbf{\\phi}, \\sigma^2 I , \\nu \\bigr ]\n",
    "\\end{eqnarray}\n",
    "\n",
    "and take the log-likelihood:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log L &=& \\log \\Gamma \\Bigl [ \\frac{\\nu + D + 1}{2} \\Bigr ] - \\frac{D+1}{2} \\log [\\nu \\pi] - \\frac{1}{2} \\log | \\Sigma| - \\log \\Gamma \\bigl [ \\frac{\\nu}{2} \\bigr ] - \\frac{\\nu + D + 1}{2} \\log \\Bigl [ 1 + \\frac{(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi})^T(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi})}{\\sigma^2 \\nu} \\Bigr ]\n",
    "\\end{eqnarray}\n",
    "\n",
    "_Note: the $D+1$-term comes from the inclusion of \"1\" to the data vector so that the bias-term can be learned.  This element increases the size of the data vectors by 1._\n",
    "\n",
    "Now, the usual steps for maximum likelihood learning follow:  _take the derivatives and solve for the parameter values that make the derivatives vanish_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log L &=& -\\frac{I}{2} \\log [2 \\pi] - \\frac{I}{2} \\log \\sigma^2 - \\frac{(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi})^T(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi})}{2 \\sigma^2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Take the derivative w.r.t $\\mathbf{\\phi}$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "0 &=& \\frac{\\partial (\\log L)}{\\partial \\mathbf{\\phi}} \\\\\n",
    "&=& \\frac{1}{2 \\sigma^2} \\bigl [ -\\mathbf{X}(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi}) -\\mathbf{X}(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi}) \\bigr ] \\\\\n",
    "&=& -\\frac{1}{\\sigma^2} \\bigl [ \\mathbf{X}(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi}) \\bigr ] \\\\\n",
    "0 &=& \\mathbf{X} \\mathbf{w} - \\mathbf{X} \\mathbf{X}^T \\mathbf{\\phi} \\\\\n",
    "\\mathbf{X} \\mathbf{X}^T \\mathbf{\\phi} &=& \\mathbf{X} \\mathbf{w} \\\\\n",
    "\\mathbf{\\phi} &=& \\bigl ( \\mathbf{X} \\mathbf{X}^T \\bigr )^{-1} \\mathbf{X} \\mathbf{w}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
