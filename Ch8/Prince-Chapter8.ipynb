{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping criteria met after 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import utilities as utils\n",
    "reload(utils)\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common import multivariate_student\n",
    "reload(multivariate_student)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "The conditional density for the discriminative model is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "Pr(w \\ | \\ \\mathbf{X} ) &=& \\text{Beta}_{w} [\\alpha, \\alpha] \\\\\n",
    "&=& \\frac{\\Gamma[2 \\alpha]}{\\Gamma[\\alpha]^2} w^{\\alpha-1}(1-w)^{\\alpha-1}, \n",
    "\\end{eqnarray}\n",
    "\n",
    "where the given constraint $\\alpha=\\beta$ has been used.\n",
    "\n",
    "The parameter $\\alpha \\in \\mathbb{R}^+$ is said to depend on the data, so one approach would be to setup a linear mapping from $\\mathbb{R}^D$ to $\\mathbb{R}^+$ as\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha = \\frac{1}{I} \\sum_{i=1}^I f(\\mathbf{\\phi}^T \\mathbf{x'}_i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\mathbf{x'}$ includes the constant 1 in the first position to allow training the bias term $\\phi_0$.  The continuously differentiable function $f$ is chosen so that $\\alpha$ can be enforced to be positive.  One option for $f$ would be a superposition of radial basis functions.\n",
    "\n",
    "Next, find the log-likelihood; this will provide the objective function to be maximized.  Assuming the datapoints are independent, it follows that:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log L &=& \\sum_{i=1}^I I \\log \\Gamma [2 \\alpha ] - 2 I \\log \\Gamma [ \\alpha ] + (\\alpha-1) \\sum_{i=1}^I \\bigl ( \\log w_i + \\log [1 - w_i ] \\bigr ) \n",
    "\\end{eqnarray}\n",
    "\n",
    "Now, the model for $\\alpha$ can be plugged in to the log-likelihood and the resulting equation can be differentiated with respect to the $\\phi_k$'s, with $k \\in {0, \\dots, D}$.  The maximum likelihood will occur where all derivatives vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Start with the conditional probability density:\n",
    "\\begin{eqnarray}\n",
    "Pr(\\mathbf{w} \\ | \\ \\mathbf{X}) &=& \\text{Stud}_\\mathbf{w} \\bigl [ \\mathbf{X}^T \\mathbf{\\phi}, \\sigma^2 I , \\nu \\bigr ]\n",
    "\\end{eqnarray}\n",
    "\n",
    "and take the log-likelihood:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log L &=& \\log \\Gamma \\Bigl [ \\frac{\\nu + D + 1}{2} \\Bigr ] - \\frac{D+1}{2} \\log [\\nu \\pi] - \\frac{1}{2} \\log | \\Sigma| - \\log \\Gamma \\bigl [ \\frac{\\nu}{2} \\bigr ] - \\frac{\\nu + D + 1}{2} \\log \\Bigl [ 1 + \\frac{(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi})^T(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi})}{\\sigma^2 \\nu} \\Bigr ]\n",
    "\\end{eqnarray}\n",
    "\n",
    "_Note: the $D+1$-term comes from the inclusion of \"1\" to the data vector so that the bias-term can be learned.  This element increases the size of the data vectors by 1._\n",
    "\n",
    "Now, the usual steps for maximum likelihood learning follow:  _take the derivatives and solve for the parameter values that make the derivatives vanish_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log L &=& -\\frac{I}{2} \\log [2 \\pi] - \\frac{I}{2} \\log \\sigma^2 - \\frac{(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi})^T(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi})}{2 \\sigma^2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Take the derivative w.r.t $\\mathbf{\\phi}$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "0 &=& \\frac{\\partial (\\log L)}{\\partial \\mathbf{\\phi}} \\\\\n",
    "&=& \\frac{1}{2 \\sigma^2} \\bigl [ -\\mathbf{X}(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi}) -\\mathbf{X}(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi}) \\bigr ] \\\\\n",
    "&=& -\\frac{1}{\\sigma^2} \\bigl [ \\mathbf{X}(\\mathbf{w} - \\mathbf{X}^T \\mathbf{\\phi}) \\bigr ] \\\\\n",
    "0 &=& \\mathbf{X} \\mathbf{w} - \\mathbf{X} \\mathbf{X}^T \\mathbf{\\phi} \\\\\n",
    "\\mathbf{X} \\mathbf{X}^T \\mathbf{\\phi} &=& \\mathbf{X} \\mathbf{w} \\\\\n",
    "\\mathbf{\\phi} &=& \\bigl ( \\mathbf{X} \\mathbf{X}^T \\bigr )^{-1} \\mathbf{X} \\mathbf{w}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "\\begin{eqnarray}\n",
    "Pr(\\mathbf{\\phi} \\ | \\ \\mathbf{X}, \\mathbf{w}) &=& \\frac{Pr(\\mathbf{w} \\ | \\ \\mathbf{X}, \\mathbf{\\phi}) Pr(\\mathbf{\\phi})}{Pr(\\mathbf{w} \\ | \\ \\mathbf{X})} \\\\\n",
    "&=& \\frac{\\text{Norm}_{\\mathbf{w}} \\bigl [ \\mathbf{X}^T \\mathbf{\\phi}, \\sigma^2 I \\bigr ] \\text{Norm}_{\\mathbf{\\phi}} \\bigl [ \\mathbf{0}, \\sigma_p^2 I \\bigr ]}{Pr(\\mathbf{w} \\ | \\ \\mathbf{X})}\n",
    "\\end{eqnarray}\n",
    "\n",
    "See Eqn. 5.18:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{Norm}_{\\mathbf{w}} \\bigl [ \\mathbf{X}^T \\mathbf{\\phi}, \\sigma^2 I \\bigr ] &=& \\kappa \\text{Norm}_{\\mathbf{\\phi}} \\Bigl [ \\frac{1}{\\sigma^2} \\Bigl ( \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T \\Bigr )^{-1} \\mathbf{X} \\mathbf{w}, \\frac{1}{\\sigma^2} \\Bigl ( \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T \\Bigr )^{-1} \\Bigr ] \\\\\n",
    "&=& \\kappa \\text{Norm}_{\\mathbf{\\phi}} \\bigl [ (\\mathbf{X} \\mathbf{X}^T)^{-1} \\mathbf{X} \\mathbf{w}, \\sigma^2 ( \\mathbf{X} \\mathbf{X}^T)^{-1} \\bigr ]\n",
    "\\end{eqnarray}\n",
    "\n",
    "Combine with original Bayes' formulation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\text{Norm}_{\\mathbf{w}} \\bigl [ \\mathbf{X}^T \\mathbf{\\phi}, \\sigma^2 I \\bigr ] \\text{Norm}_{\\mathbf{\\phi}} \\bigl [ \\mathbf{0}, \\sigma_p^2 I \\bigr ]}{Pr(\\mathbf{w} \\ | \\ \\mathbf{X})} &=& \\frac{\\kappa \\text{Norm}_{\\mathbf{\\phi}} \\bigl [ (\\mathbf{X} \\mathbf{X}^T)^{-1} \\mathbf{X} \\mathbf{w}, \\sigma^2 ( \\mathbf{X} \\mathbf{X}^T)^{-1} \\bigr ] \\text{Norm}_{\\mathbf{\\phi}} \\bigl [ \\mathbf{0}, \\sigma_p^2 I \\bigr ]}{Pr(\\mathbf{w} \\ | \\ \\mathbf{X})}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Use Eqn. 5.14 to combine:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\kappa \\text{Norm}_{\\mathbf{\\phi}} \\bigl [ (\\mathbf{X} \\mathbf{X}^T)^{-1} \\mathbf{X} \\mathbf{w}, \\sigma^2 ( \\mathbf{X} \\mathbf{X}^T)^{-1} \\bigr ] \\text{Norm}_{\\mathbf{\\phi}} \\bigl [ \\mathbf{0}, \\sigma_p^2 I \\bigr ]}{Pr(\\mathbf{w} \\ | \\ \\mathbf{X})} &=& \\frac{\\bar \\kappa}{Pr(\\mathbf{w} \\ | \\ \\mathbf{X})} \\text{Norm}_{\\mathbf{\\phi}} \\Bigl [ \\Bigl ( \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T + \\frac{1}{\\sigma_p^2} I \\Bigr )^{-1} \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T  \\bigl ( \\mathbf{X} \\mathbf{X}^T \\bigr )^{-1} \\mathbf{X} \\mathbf{w}, \\Bigl ( \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T + \\frac{1}{\\sigma_p^2} I \\Bigr )^{-1} \\Bigr ] \\\\\n",
    "&=& \\frac{\\bar \\kappa}{Pr(\\mathbf{w} \\ | \\ \\mathbf{X})} \\text{Norm}_{\\mathbf{\\phi}} \\Bigl [ \\Bigl ( \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T + \\frac{1}{\\sigma_p^2} I \\Bigr )^{-1} \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{w}, \\Bigl ( \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T + \\frac{1}{\\sigma_p^2} I \\Bigr )^{-1} \\Bigr ]\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\bar \\kappa$ combines constants from application of Eqns. 5.18 and 5.14.  The constant $\\frac{\\bar \\kappa}{Pr(\\mathbf{w} \\ | \\ \\mathbf{X})}$ must be equal to 1 in order for the expression to be a valid probability distribution.  Now, the desired form is found:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "Pr(\\mathbf{\\phi} \\ | \\ \\mathbf{X}, \\mathbf{w}) &=& \\text{Norm}_{\\mathbf{\\phi}} \\Bigl [ \\frac{1}{\\sigma^2} A^{-1} \\mathbf{X} \\mathbf{w}, A^{-1} \\Bigr ] \\\\\n",
    "A &=& \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T + \\frac{1}{\\sigma_p^2} I\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "As mentioned in the solutions manual, tackling this problem directly would be a pain.  Instead, consider the following for a new datapoint:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "w^* &=& \\mathbf{x^*}^T \\mathbf{\\phi} + \\varepsilon, \\ \\varepsilon \\sim \\text{Norm}_h [ 0, \\sigma^2] \\\\\n",
    "\\mathbf{\\phi} &=& \\frac{1}{\\sigma^2} A^{-1} \\mathbf{X} \\mathbf{w} + \\mathbf{\\alpha}, \\ \\mathbf{\\alpha} \\sim \\text{Norm}_\\mathbf{h} [\\mathbf{0}, \\mathbf{A}^{-1}] \\\\\n",
    "A &=& \\frac{1}{\\sigma^2} \\mathbf{X} \\mathbf{X}^T + \\frac{1}{\\sigma_p^2} I\n",
    "\\end{eqnarray}\n",
    "\n",
    "The $h$ and $\\mathbf{h}$ are hidden variables that are used for the random perturbations of the world state $w^*$ and the weight vector $\\phi$, respectively.  Take a look at Eqn. 8.12 if the form above looks strange; the results above are a simple consequence of the form of the probability distributions inside the integral on the second line.  Now, to show that $w^*$ is, in fact, drawn from the desired distribution, look at expectations (taken w.r.t the hidden variables):\n",
    "\n",
    "\\begin{eqnarray}\n",
    "E[ w^* ] &=& \\mathbf{x^*}^T E [ \\mathbf{\\phi}] + E[\\varepsilon] \\\\\n",
    "&=& \\frac{1}{\\sigma^2} \\mathbf{x^*}^T A^{-1} \\mathbf{X} \\mathbf{w} + 0 \\\\\n",
    "&=& \\frac{1}{\\sigma^2} \\mathbf{x^*}^T A^{-1} \\mathbf{X} \\mathbf{w} \\\\\n",
    "&\\triangleq& \\mu_{w^*} \\\\\n",
    "E[(w^*)^2] &=& \\mathbf{x^*}^T E [ \\mathbf{\\phi} \\mathbf{\\phi}^T ] \\mathbf{x^*} + E [\\varepsilon^2 ] \\\\\n",
    "&=& \\mathbf{x^*}^T E [ \\mathbf{\\phi} \\mathbf{\\phi}^T ] \\mathbf{x^*} + \\sigma^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "since $\\varepsilon = \\varepsilon - E[\\varepsilon]$.  _Note: the desired relationship for the mean has already been found above._\n",
    "\n",
    "Now, for $E [ \\mathbf{\\phi} \\mathbf{\\phi}^T ]$, note that (see Ch. 3):\n",
    "\\begin{eqnarray}\n",
    "E[(\\mathbf{\\phi} - E[\\mathbf{\\phi}])(\\mathbf{\\phi} - E[\\mathbf{\\phi}])^T] &=& E [ \\mathbf{\\phi} \\mathbf{\\phi}^T ] - E [ \\mathbf{\\phi} ] E [ \\mathbf{\\phi}^T] \\\\\n",
    "(\\implies) A^{-1} &=& E [ \\mathbf{\\phi} \\mathbf{\\phi}^T ] - \\frac{1}{\\sigma^4} A^{-1} \\mathbf{X} \\mathbf{w} \\mathbf{w}^T \\mathbf{X}^T A^{-1} \\\\\n",
    "E [ \\mathbf{\\phi} \\mathbf{\\phi}^T ] &=& A^{-1} + \\frac{1}{\\sigma^4} A^{-1} \\mathbf{X} \\mathbf{w} \\mathbf{w}^T \\mathbf{X}^T A^{-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Putting all of this information together, we have:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "E[(w^*-E[w^*])^2] &=& E[(w^*)^2] - E[w^*]^2 \\\\\n",
    "&=& \\mathbf{x^*}^T \\Bigl ( A^{-1} + \\frac{1}{\\sigma^4} A^{-1} \\mathbf{X} \\mathbf{w} \\mathbf{w}^T \\mathbf{X}^T A^{-1} \\Bigr ) \\mathbf{x^*} + \\sigma^2 - \\frac{1}{\\sigma^4} \\mathbf{x^*}^T A^{-1} \\mathbf{X} \\mathbf{w} \\mathbf{w}^T \\mathbf{X}^T A^{-1} \\mathbf{x^*} \\\\\n",
    "&=& \\mathbf{x^*}^T A^{-1} \\mathbf{x^*} + \\sigma^2 \\\\\n",
    "&\\triangleq& \\Sigma_{w^*}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Therefore, we have shown the desired relationship $w^* \\sim \\text{Norm}[ \\mu_{w^*}, \\Sigma_{w^*} ]$ with:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mu_{w^*} &\\triangleq& \\frac{1}{\\sigma^2} \\mathbf{x^*}^T A^{-1} \\mathbf{X} \\mathbf{w} \\\\\n",
    "\\Sigma_{w^*} &\\triangleq& \\mathbf{x^*}^T A^{-1} \\mathbf{x^*} + \\sigma^2\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
